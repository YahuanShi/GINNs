{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/system/user/radler/repos/objgen/notebooks', '/system/apps/userenv/radler/ginn/lib/python312.zip', '/system/apps/userenv/radler/ginn/lib/python3.12', '/system/apps/userenv/radler/ginn/lib/python3.12/lib-dynload', '', '/system/apps/userenv/radler/ginn/lib/python3.12/site-packages', '/system/user/radler/repos/objgen']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.curdir, '..')))\n",
    "print(sys.path)\n",
    "import trimesh\n",
    "import k3d\n",
    "import numpy as np\n",
    "from util.visualization.utils_mesh import get_watertight_mesh_for_latent\n",
    "import torch\n",
    "import numpy as np\n",
    "import k3d\n",
    "import trimesh\n",
    "from tqdm import tqdm\n",
    "from utils import get_model, get_model_path_via_wandb_id_from_fs, get_stateless_net_with_partials, set_else_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "device = 'cuda:1'\n",
    "cfg = {\n",
    "    'layers': [128, 128, 128],\n",
    "    'w0': 1.0,\n",
    "    'w0_initial': 18,\n",
    "    'wire_scale': 6,\n",
    "    'model': 'cond_wire',\n",
    "    'nx': 3,\n",
    "    'ny': 1,\n",
    "    'nz': 1,\n",
    "    'problem': 'simjeb',\n",
    "    'simjeb_root_dir': '../GINN/simJEB/data',\n",
    "    'envelope_sample_from': 'exterior',\n",
    "    'n_points_domain': 2048,  # used for eikonal loss\n",
    "    'n_points_envelope': 16384,\n",
    "    'n_points_interfaces': 4096,\n",
    "    'n_points_normals': 4096,\n",
    "    'mc_resolution': 128,\n",
    "    'bounds': torch.from_numpy(np.load('../GINN/simJEB/data/bounds.npy')).float().to(device),\n",
    "    'device': device,\n",
    "    'ginn_bsize': 2,\n",
    "    'surf_pts_recompute_every_n_epochs': 1,\n",
    "    'surf_pts_nof_points': 8192, # 32768  ## nof points for initializing the flow to surface\n",
    "    'surf_pts_lr': 0.01, ## learning rate for non-Newton optimizer\n",
    "    'surf_pts_n_iter': 10, # iterations of surface flow\n",
    "    'surf_pts_prec_eps': 1.0e-3,  ## precision threshold for early stopping surface flow and filtering the points \n",
    "    'surf_pts_converged_interval': 1, ## how often to check the convergence\n",
    "    'surf_pts_use_newton': True, ## whether to use Newton iteration or Adam\n",
    "    'surf_pts_newton_clip': 0.15, ## magnitude for clipping the Newton update\n",
    "    'surf_pts_inflate_bounds_amount': 0.05, ## inflate the (otherwise tight) bounding box by this fraction\n",
    "    'surf_pts_uniform_n_iter': 10, ## nof iterations for repelling the points\n",
    "    'surf_pts_uniform_nof_neighbours': 16, ## nof neighbors for knn\n",
    "    'surf_pts_uniform_stepsize': 0.75, ## step size for the repelling update\n",
    "    'surf_pts_uniform_n_iter_reproj': 5, ## nof Newton-iterations for reprojecting the points\n",
    "    'surf_pts_uniform_prec_eps': 1.0e-3, ## precision for reprojection (similar to above)\n",
    "    'surf_pts_uniform_min_count': 1000, ## minimum number of points to redistribute. Less than this is meaningless\n",
    "    'surf_pts_surpress_tqdm': True,\n",
    "    'surf_pts_uniform_reproject_surpress_tqdm': True,\n",
    "    'fig_show': False,\n",
    "    'fig_save': False,\n",
    "    'fig_wandb': False,\n",
    "}\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model at /system/user/publicwork/radler/ginndata/saved_models/cond_wire/2024_09_04__14_45_35-7bnersho/2024_09_04__14_45_35-7bnersho-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "########## MODELS ##########\n",
    "\n",
    "## No smoothing\n",
    "key = '7bnersho'\n",
    "model_path = get_model_path_via_wandb_id_from_fs(key)\n",
    "z = torch.tensor([[0.0], [0.1]], device=device)\n",
    "\n",
    "########## END MODELS ##########\n",
    "\n",
    "## MODEL\n",
    "# activation = get_activation(config.get('activation', None))\n",
    "model = get_model(cfg).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "nep = get_stateless_net_with_partials(model, cfg['nz'])\n",
    "params, f, vf_x, vf_xx = nep.params_, nep.f_, nep.vf_x_, nep.vf_xx_\n",
    "\n",
    "## visualize shapes for a range of z\n",
    "meshes = []\n",
    "for z_ in tqdm(z): ## do marching cubes for every z\n",
    "    meshes.append(get_watertight_mesh_for_latent(f, params, z_, cfg['bounds'], cfg['mc_resolution'], device, surpress_watertight=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GINN import problem_sampler\n",
    "from GINN.visualize.plotter_dummy import DummyPlotHelper\n",
    "from GINN.helpers.timer_helper import TimerHelper\n",
    "from GINN.helpers.mp_manager import MPManager\n",
    "\n",
    "\n",
    "mpm = MPManager(cfg)\n",
    "t_helper = TimerHelper(cfg, lock=mpm.get_lock())\n",
    "mpm.set_timer_helper(t_helper)  # avoid circular dependencies\n",
    "p_sampler = problem_sampler.ProblemSampler(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move done in 0.009 s\n",
      "move done in 0.022 s\n",
      "reproject done in 0.104 s\n",
      "move done in 0.009 s\n",
      "move done in 0.025 s\n",
      "reproject done in 0.103 s\n",
      "move done in 0.008 s\n",
      "move done in 0.023 s\n",
      "reproject done in 0.101 s\n",
      "move done in 0.009 s\n",
      "move done in 0.022 s\n",
      "reproject done in 0.101 s\n",
      "move done in 0.009 s\n",
      "move done in 0.022 s\n",
      "reproject done in 0.103 s\n",
      "move done in 0.009 s\n",
      "move done in 0.023 s\n",
      "reproject done in 0.092 s\n",
      "move done in 0.008 s\n",
      "move done in 0.020 s\n",
      "reproject done in 0.071 s\n",
      "move done in 0.007 s\n",
      "move done in 0.020 s\n",
      "reproject done in 0.065 s\n",
      "move done in 0.010 s\n",
      "move done in 0.019 s\n",
      "reproject done in 0.066 s\n",
      "move done in 0.007 s\n",
      "move done in 0.020 s\n",
      "reproject done in 0.066 s\n",
      "found pts in 1.369 s\n",
      "move done in 0.009 s\n",
      "move done in 0.017 s\n",
      "reproject done in 0.073 s\n",
      "move done in 0.008 s\n",
      "move done in 0.018 s\n",
      "reproject done in 0.069 s\n",
      "move done in 0.008 s\n",
      "move done in 0.020 s\n",
      "reproject done in 0.066 s\n",
      "move done in 0.009 s\n",
      "move done in 0.019 s\n",
      "reproject done in 0.061 s\n",
      "move done in 0.008 s\n",
      "move done in 0.020 s\n",
      "reproject done in 0.068 s\n",
      "move done in 0.008 s\n",
      "move done in 0.019 s\n",
      "reproject done in 0.061 s\n",
      "move done in 0.006 s\n",
      "move done in 0.018 s\n",
      "reproject done in 0.063 s\n",
      "move done in 0.009 s\n",
      "move done in 0.020 s\n",
      "reproject done in 0.067 s\n",
      "move done in 0.009 s\n",
      "move done in 0.010 s\n",
      "reproject done in 0.064 s\n",
      "move done in 0.009 s\n",
      "move done in 0.010 s\n",
      "reproject done in 0.069 s\n",
      "redistributed pts in 0.919 s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from models.point_wrapper import PointWrapper\n",
    "from train.losses import get_gauss_curvature, get_mean_curvature_normalized\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from util.misc import set_and_true, set_else_default\n",
    "from util.sample_utils import inflate_bounds, precompute_sample_grid\n",
    "\n",
    "class ShapeBoundaryHelper:\n",
    "    \n",
    "    def __init__(self, config, netp, mp_manager: MPManager, plot_helper, timer_helper: TimerHelper, x_interface, device):\n",
    "        self.config = config\n",
    "        self.netp = netp.detach()\n",
    "        self.mpm = mp_manager\n",
    "        self.logger = logging.getLogger('surf_pts_helper')\n",
    "        self.plotter = plot_helper\n",
    "        self.timer_helper = timer_helper\n",
    "        self.bounds = self.config['bounds'].to(device)\n",
    "        self.bounds = inflate_bounds(self.bounds, amount=set_else_default('surf_pts_inflate_bounds_amount', self.config, 0.05))\n",
    "        self.grid_find_surface, self.grid_dist_find_surface = precompute_sample_grid(self.config['surf_pts_nof_points'], self.bounds)\n",
    "        \n",
    "        self.record_time = timer_helper.record        \n",
    "        self.p_surface = None\n",
    "        self.x_interface = x_interface\n",
    "        self.knn_k = set_else_default('surf_pts_uniform_nof_neighbours', config, 16)\n",
    "        # NOTE: more neighbors pushes the points more to edges, so might be favourable for smoothness \n",
    "    \n",
    "    def get_surface_pts(self, z):\n",
    "        success, p_surface = self._get_and_plot_surface_flow(z)\n",
    "        if not success:\n",
    "            return None, None\n",
    "\n",
    "        if len(p_surface)>set_else_default('surf_pts_uniform_min_count', self.config, 1000): \n",
    "            ## Stop redistributing if there are not enough points.\n",
    "            ## Better return failure so that the integrals don't have a high variance\n",
    "            success, p_surface = self.resample(p_surface, z, num_iters=set_else_default('surf_pts_uniform_n_iter', self.config, 10))\n",
    "\n",
    "        if not success:\n",
    "            return None, None\n",
    "\n",
    "        weights_surf_pts = torch.ones(len(p_surface)) / p_surface.data.shape[0]\n",
    "        dist = torch.min(torch.norm(p_surface.data[:, None, :] - self.x_interface[None, :, :], dim=2), dim=1)[0]\n",
    "        if set_and_true('reweigh_surface_pts_close_to_interface', self.config):\n",
    "            dist = torch.min(torch.norm(p_surface.data[:, None, :] - self.x_interface[None, :, :], dim=2), dim=1)[0]            \n",
    "            assert False, 'break here'\n",
    "            dist = torch.clamp(dist, max=self.config['reweigh_surface_pts_close_to_interface_cutoff'])\n",
    "            weights_surf_pts = torch.pow(dist, self.config['reweigh_surface_pts_close_to_interface_power'])\n",
    "            weights_surf_pts = weights_surf_pts / weights_surf_pts.sum()  ## normalize to sum to 1\n",
    "\n",
    "        if set_and_true('plot_surface_points', self.config):\n",
    "            y_x_surf = self.netp.vf_x(p_surface.data, p_surface.z_in(z)).squeeze(1)\n",
    "            y_xx_surf = self.netp.vf_xx(p_surface.data, p_surface.z_in(z)).squeeze(1)\n",
    "            \n",
    "            mean_curvatures = get_mean_curvature_normalized(y_x_surf, y_xx_surf)\n",
    "            gauss_curvatures = get_gauss_curvature(y_x_surf, y_xx_surf)\n",
    "            E_strain = (2*mean_curvatures)**2 - 2*gauss_curvatures\n",
    "            E_strain = torch.log(E_strain + 1e-3)  ## log to make the values more interpretable\n",
    "\n",
    "            self.mpm.plot(self.plotter.plot_shape_and_points, 'plot_surface_points', \n",
    "                                arg_list=[p_surface.detach().cpu().numpy(), 'Surface points', E_strain.detach().cpu().numpy()])\n",
    "        return p_surface, weights_surf_pts\n",
    "    \n",
    "    def _get_and_plot_surface_flow(self, z):\n",
    "        \n",
    "        with self.record_time('cp_helper: flow_to_surface_points'):\n",
    "            p = self.get_grid_starting_pts(self.grid_find_surface, self.grid_dist_find_surface)\n",
    "            success, tup = self.flow_to_surface_pts(p, z, \n",
    "                lr=self.config['surf_pts_lr'],\n",
    "                n_iter=self.config['surf_pts_n_iter'],\n",
    "                plot_descent=self.plotter.do_plot('plot_surface_descent'),\n",
    "                use_newton=self.config['surf_pts_use_newton'],\n",
    "                surpress_tqdm=set_else_default('surf_pts_surpress_tqdm', self.config, False),\n",
    "                )\n",
    "\n",
    "        if not success:\n",
    "            self.logger.debug(f'No surface points found')\n",
    "            return False, None        \n",
    "        p_surface, x_path_over_iters = tup\n",
    "        if self.plotter.do_plot('plot_surface_descent'):\n",
    "            self.mpm.plot(self.plotter.plot_descent_trajectories, 'plot_surface_descent', [p.detach().cpu().numpy(), x_path_over_iters.cpu().numpy()])\n",
    "\n",
    "        return True, p_surface\n",
    "    \n",
    "    def flow_to_surface_pts(self, p, z, lr, n_iter, plot_descent, filter_thr=None, newton_clip=None, min_count=None, use_sgd=False, use_newton=True, surpress_tqdm=False):\n",
    "        \"\"\"\n",
    "        A simple optimization loop to let starting points p flow to zero.\n",
    "        NOTE: Adam/SGD is kept for historic reasons, but going forward we might want to\n",
    "        either split or remove it as the current code is a bit unreadable.\n",
    "        The main difference between Adam and Newton:\n",
    "        Adam requires to register the variables, which stay fixed size: filtering just selects a subset for evaluation and updating.\n",
    "        Newton update is manual, so we can always throw away points.\n",
    "        \"\"\"\n",
    "\n",
    "        ## Filter far away from surface so we get a more uniform distribution and need less iterations\n",
    "        # y = self.netp.f(p.data, p.z_in(z)).squeeze(1)\n",
    "        # init_mask = torch.abs(y) < 5e-2\n",
    "        # p = p.select_w_mask(incl_mask=init_mask)\n",
    "\n",
    "        ## Initialize parameters\n",
    "        if filter_thr is None:\n",
    "            filter_thr = set_else_default('surf_pts_prec_eps', self.config, 1e-3)\n",
    "        if newton_clip is None:\n",
    "            newton_clip = set_else_default('surf_pts_newton_clip', self.config, 0.15)\n",
    "        if min_count is None:\n",
    "            min_count = set_else_default('surf_pts_uniform_min_count', self.config, 100)\n",
    "\n",
    "        ## Initialize plotting\n",
    "        x_path_over_iters = None\n",
    "        if plot_descent:\n",
    "            x_path_over_iters = torch.full([n_iter + 1, len(p), self.config['nx']], torch.nan)\n",
    "            idxs_in_orig = torch.arange(0, len(p))\n",
    "\n",
    "        ## Initialize points and optimizer\n",
    "        if use_newton:\n",
    "            p_in = p\n",
    "        else:\n",
    "            p.data.requires_grad = True\n",
    "            opt = torch.optim.Adam([p.data], lr=lr)\n",
    "            if use_sgd:\n",
    "                opt = torch.optim.SGD([p.data], lr=lr)\n",
    "    \n",
    "        ## Iterate\n",
    "        for i in (pbar := trange(n_iter, disable=surpress_tqdm)):\n",
    "\n",
    "            ## Mask\n",
    "            if use_newton:\n",
    "                out_mask = get_is_out_mask(p_in.data, self.bounds)\n",
    "                p_in = p_in.select_w_mask(incl_mask=~out_mask)\n",
    "                if plot_descent:\n",
    "                    idxs_in_orig = idxs_in_orig[~out_mask]\n",
    "                    x_path_over_iters[i][idxs_in_orig] = p_in.data.detach()\n",
    "            else:\n",
    "                opt.zero_grad()\n",
    "                out_mask = get_is_out_mask(p.data, self.bounds)\n",
    "                p_in = p.select_w_mask(incl_mask=~out_mask)\n",
    "                if plot_descent:\n",
    "                    x_path_over_iters[i] = p.data.detach()\n",
    "\n",
    "            if len(p_in) == 0:\n",
    "                self.logger.debug(f'No surf_pts_n_iter points found in the domain')\n",
    "                return False, None\n",
    "\n",
    "            ## Main update\n",
    "            if use_newton:\n",
    "                with torch.no_grad():\n",
    "                    z_ = p_in.z_in(z)\n",
    "                    y = self.netp.f(p_in.data, z_).squeeze(1)\n",
    "                    y_x = self.netp.vf_x(p_in.data, z_).squeeze(1)\n",
    "                    update = y_x * (torch.clip(y, -newton_clip, newton_clip)/y_x.norm(dim=1))[:,None]\n",
    "                    p_in.data = p_in.data - update\n",
    "\n",
    "                    ## For compatibility with remaining code\n",
    "                    y_in = y\n",
    "\n",
    "                    ## Logging\n",
    "                    if not surpress_tqdm:\n",
    "                        loss = y_in.square().mean()\n",
    "                        pbar.set_description(f\"Flow to surface points: {len(p_in)}/{len(p)}; {loss.item():.2e}\")\n",
    "            else:\n",
    "                y_in = self.netp.f(p_in.data, p_in.z_in(z)).squeeze(1)  ## [bx]\n",
    "                \n",
    "                # L2 loss works better than L1 loss\n",
    "                loss = y_in.square().mean()\n",
    "                if torch.isnan(loss):\n",
    "                    self.logger.debug(f'Early stop \"Finding surface points\" at it {i} due to nan loss')\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                if not surpress_tqdm:\n",
    "                    pbar.set_description(f\"Flow to surface points: {len(p_in)}/{len(p)}; {loss.item():.2e}\")\n",
    "            \n",
    "            ## Early stopping\n",
    "            if i % self.config['surf_pts_converged_interval'] == 0:\n",
    "                # stop if |points| < thresh\n",
    "                if (torch.abs(y_in) < self.config['surf_pts_prec_eps']).all():\n",
    "                    self.logger.debug(f'Early stop \"Finding surface points\" at it {i}')\n",
    "                    break\n",
    "                \n",
    "        \n",
    "        ## Filter non-converged points\n",
    "        converged_mask = torch.abs(y_in) < filter_thr\n",
    "        p_in = p_in.select_w_mask(incl_mask=converged_mask)\n",
    "\n",
    "        ## Exit early if no points are left\n",
    "        if len(p_in)<min_count:\n",
    "            self.logger.debug(f'Only {len(p_in)} surface points found, not continuing')\n",
    "            return False, None\n",
    "\n",
    "        ## Handle the last iteration of plotting\n",
    "        if plot_descent:\n",
    "            if use_newton:\n",
    "                idxs_in_orig = idxs_in_orig[converged_mask]\n",
    "                x_path_over_iters[i+1][idxs_in_orig] = p_in.data.detach()\n",
    "            else:\n",
    "                x_path_over_iters[i+1] = p.data.detach()\n",
    "            x_path_over_iters = x_path_over_iters[:i+2] ## remove the unfilled part due to early stopping\n",
    "        \n",
    "        ## Disable gradient tracking for Adam\n",
    "        if not use_newton:\n",
    "            p_in = p_in.detach()\n",
    "            p_in.data.requires_grad = False\n",
    "\n",
    "        return True, (p_in, x_path_over_iters)\n",
    "\n",
    "    def get_normals(self, p, z, invert=False):\n",
    "        f_x = self.netp.vf_x(p.data, p.z_in(z)).squeeze(1)  ## [bx nx]\n",
    "        if invert:\n",
    "            f_x = -f_x\n",
    "        p_normals = PointWrapper(f_x, map=p.get_map())\n",
    "        p_normals.data = F.normalize(p_normals.data, dim=-1)\n",
    "        return p_normals\n",
    "        \n",
    "    def get_nn_idcs(self, x, k):\n",
    "        dist = torch.cdist(x, x, compute_mode='use_mm_for_euclid_dist')\n",
    "        # dist = (x.unsqueeze(1) - x.unsqueeze(0)).norm(dim=-1)\n",
    "        idcs = dist.argsort(dim=-1)[:, 1:k+1]\n",
    "        return idcs\n",
    "    \n",
    "    def resample(self, points_init, z, num_iters=0, debug=True):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        ## Initialize parameters\n",
    "        n_iter_reproj = set_else_default('surf_pts_uniform_n_iter_reproj', self.config, 5)\n",
    "        filter_thr_reproj = set_else_default('surf_pts_uniform_filter_thr_reproj', self.config, 1e-3) ## lower thr requires more n_iter\n",
    "        stepsize = set_else_default('surf_pts_uniform_stepsize', self.config, 0.75) ## .75 worked well with 8 and 16 nns\n",
    "\n",
    "        for i_iter in range(num_iters):\n",
    "            if debug:\n",
    "                if i_iter>0:\n",
    "                    self.logger.debug(f'iter: {i_iter} \\t density: {density_w.mean().item():.3f} \\t nof pts: {len(points_init)}')\n",
    "\n",
    "            for i_shape in range(len(points_init.get_map())):\n",
    "\n",
    "                start_t = time.time()\n",
    "                points = points_init.pts_of_shape(i_shape)\n",
    "\n",
    "                normals_init = self.get_normals(points_init, z)\n",
    "                normals = normals_init.pts_of_shape(i_shape)\n",
    "                num_points = points.shape[0]\n",
    "                \n",
    "                ## NOTE: not sure if this should be recomputed every iteration if the nof points doesn't change much\n",
    "                diag = (points.view(-1, 3).max(dim=0).values - points.view(-1, 3).min(0).values).norm().item()\n",
    "                if diag < 1e-6: ## Fail if the diagonal is too small\n",
    "                    return False, None\n",
    "                inv_sigma_spatial = num_points / diag\n",
    "\n",
    "                knn_indices = self.get_nn_idcs(points, self.knn_k) # [n_points, k]\n",
    "                knn_nn = points[knn_indices] # [n_points, k, 3]                \n",
    "                knn_diff = points.unsqueeze(1) - knn_nn  # [n_points, k, 3]\n",
    "                knn_dists_sq = torch.sum(knn_diff**2, dim=-1)  # [n_points, k]\n",
    "                spatial_w = torch.exp(-knn_dists_sq * inv_sigma_spatial)  # [n_points, k]\n",
    "                move = torch.sum(spatial_w[..., None] * knn_diff, dim=-2)\n",
    "\n",
    "                if debug:\n",
    "                    ## Store the previous points for debugging\n",
    "                    density_w = torch.sum(spatial_w, dim=-1, keepdim=True)  # [n_points, 1] ## NOTE: can change sum to mean to make invariant to number of neighbors \n",
    "\n",
    "                ## Project the move onto the tangential plane\n",
    "                move -= (move * normals) * normals\n",
    "                ## Scale the update\n",
    "                move *= stepsize ## the update size is a hyperparameter. Larger steps needs better reprojection\n",
    "\n",
    "                ## Update the points\n",
    "                points += move \n",
    "                points_init.set_pts_of_shape(i_shape, points)\n",
    "                \n",
    "                print(f'move done in {time.time() - start_t:.3f} s')\n",
    "            \n",
    "            start_t = time.time()\n",
    "            ## Reproject\n",
    "            ## NOTE: the majoriy of time is spent here\n",
    "            success, ret = self.flow_to_surface_pts(\n",
    "                points_init,\n",
    "                z,\n",
    "                lr=None,\n",
    "                n_iter=n_iter_reproj,\n",
    "                plot_descent=False, \n",
    "                use_newton=True, \n",
    "                surpress_tqdm=set_else_default('surf_pts_uniform_reproject_surpress_tqdm', self.config, True),\n",
    "                filter_thr=filter_thr_reproj,\n",
    "                )\n",
    "            if success:\n",
    "                points_init, _ = ret\n",
    "            else:\n",
    "                self.logger.debug(\"No points left after reprojection. Try decreasning the update size, increasing the number of reprojection iterations or decreasing the filtering threshold\")\n",
    "                return False, None \n",
    "            if len(points_init) < set_else_default('surf_pts_uniform_min_count', self.config, 1000):\n",
    "                ## Stop redistributing if there are not enough points.\n",
    "                ## Better return failure so that the integrals don't have a high variance\n",
    "                False, None\n",
    "                \n",
    "            print(f'reproject done in {time.time() - start_t:.3f} s')\n",
    "        \n",
    "        return True, points_init\n",
    "\n",
    "\n",
    "    def get_grid_starting_pts(self, x_grid, grid_dist):\n",
    "        '''\n",
    "        Create grid once at the beginning.\n",
    "        Translate the grid by a random offset.\n",
    "        '''\n",
    "        ## Translate the grid by a random offset\n",
    "        xc_offset = torch.rand((self.config['ginn_bsize'], self.config['nx'])) * grid_dist  # bz nx\n",
    "\n",
    "        # x_grid: [n_points nx]\n",
    "        x = x_grid.unsqueeze(0) + xc_offset.unsqueeze(1)  # bz n_points nx\n",
    "\n",
    "        ## Translate each point by a random offset\n",
    "        x += torch.randn(x_grid.shape) * grid_dist / 3\n",
    "\n",
    "        return PointWrapper.create_from_equal_bx(x)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "boundary_helper = ShapeBoundaryHelper(cfg, nep, mp_manager=mpm, plot_helper=DummyPlotHelper(), timer_helper=t_helper, x_interface=p_sampler.sample_from_interface()[0], device=device)\n",
    "p_surface, _ = boundary_helper.get_surface_pts(z)\n",
    "print(f\"found pts in {(time.time() - t0):.3f} s\"); t0 = time.time()\n",
    "p_orig = p_surface.data.clone().detach()\n",
    "# p_resampled, move, knn_diff, density_w, knn_dists_sq, knn_indices, points_prev = boundary_helper.resample(p_surface, z, num_iters=10, debug=True)\n",
    "success, p_resampled = boundary_helper.resample(p_surface, z, num_iters=10, debug=True)\n",
    "print(f\"redistributed pts in {(time.time() - t0):.3f} s\"); t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/system/apps/userenv/radler/ginn/lib/python3.12/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"int32\" does not match required type \"uint32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ace83d0e1994684b80f6b0afae0528f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "verts, faces = meshes[0]\n",
    "color = 0xbbbbbb\n",
    "\n",
    "\n",
    "fig = k3d.plot(height=1000) #, camera_fov=1.0)\n",
    "n_cols = 2\n",
    "\n",
    "bounds = cfg['bounds'].cpu().numpy()\n",
    "shape_grid = 1.5*(bounds[:,1] - bounds[:,0]) ## distance between the shape grid for plotting\n",
    "\n",
    "for i_shape in range(len(meshes)):\n",
    "    i_col = (i_shape  % n_cols)\n",
    "    i_row = (i_shape // n_cols)\n",
    "    verts, faces = meshes[i_shape]\n",
    "    group = f'Shape {i_shape}'\n",
    "    fig += k3d.mesh(verts, faces, group=group, color=color, side='double', flat_shading=False, opacity=1.0, name=f\"Shape_{i_shape}\", translation=[0, shape_grid[1]*i_col, shape_grid[2]*i_row])\n",
    "    fig += k3d.points(p_resampled.pts_of_shape(i_shape).cpu().numpy(), point_size=0.003, name=f'pts_{i_shape}', translation=[0, shape_grid[1]*i_col, shape_grid[2]*i_row])\n",
    "    \n",
    "\n",
    "# fig += k3d.mesh(verts, faces, color=color, side='double', flat_shading=False, opacity=1.0, name=\"Shape\")\n",
    "\n",
    "## Debug nearest-neighbor\n",
    "# knn_edges = torch.stack((\n",
    "#     torch.arange(knn_indices.shape[0]).repeat(knn_indices.shape[1],1).T, \n",
    "#     knn_indices)).flatten(start_dim=1).T.cpu().numpy()\n",
    "# fig += k3d.lines(points_prev.cpu().numpy(), knn_edges, indices_type='segment', shader=\"simple\", width=0.01, color=0x0000ff)\n",
    "\n",
    "## Debug density\n",
    "# fig += k3d.points(points_prev.cpu().numpy(), point_size=0.003, attribute=density_w.detach().cpu().numpy(), name='pts') ## can also plotmean_knn_dists\n",
    "\n",
    "## Debug directions\n",
    "# fig += k3d.vectors(points_prev.cpu().numpy(), move.detach().cpu().numpy(), color=0x0000ff, line_width=0.0001, head_size=0.01, name='move')\n",
    "\n",
    "## Debug resampling\n",
    "# fig += k3d.points(p_orig.detach().cpu().numpy(), point_size=0.003, color=0x00ff00, name='pts')\n",
    "# fig += k3d.points(p_resampled.data.detach().cpu().numpy(), point_size=0.003, color=0xff0000, name='resampled')\n",
    "# fig += k3d.points(p_surface.data.detach().cpu().numpy(), point_size=0.003, color=0x0000ff, name='surface')\n",
    "# fig += k3d.points(p_reproj.data.detach().cpu().numpy(), point_size=0.003, color=0x0000ff, name='reproj')\n",
    "# fig += k3d.vectors(p_resampled.detach().cpu().numpy(), (p_resampled.data - p_reproj.data).detach().cpu().numpy(), color=0x0000ff, line_width=0.0001, head_size=0.01, name='move')\n",
    "\n",
    "## Plot trajectories\n",
    "def trajectories_to_verts_inds(trajectories):\n",
    "    vertices = trajectories.reshape(-1,3).cpu().numpy()\n",
    "    # K steps, P points, 3 coordinates\n",
    "    K, P, _ = trajectories.shape\n",
    "    start_indices = np.arange((K - 1) * P)\n",
    "    end_indices = start_indices + P\n",
    "    # Stack them to form the pairs of indices for the lines\n",
    "    indices = np.vstack([start_indices, end_indices]).T\n",
    "    return vertices, indices\n",
    "## Filter trajectories\n",
    "# converged_mask = x_path_over_iters[-1,:,0].isnan()\n",
    "# trajs = x_path_over_iters #x_path_over_iters[:,~converged_mask][:,::1]\n",
    "# fig += k3d.lines(*trajectories_to_verts_inds(trajs), indices_type='segment', shader=\"simple\", width=0.01, color=0x0000ff)\n",
    "# fig += k3d.points(p_in.data.detach().cpu().numpy(), point_size=0.003, color=0x00ff00, name='pts')\n",
    "# fig += k3d.points(x_path_over_iters[0].cpu().numpy(), point_size=0.003, color=0xff0000, name='pts_init')\n",
    "\n",
    "\n",
    "## Bounds\n",
    "# from k3d import platonic\n",
    "# x_min, x_max, y_min, y_max, z_min, z_max = boundary_helper.bounds.flatten().cpu().numpy()\n",
    "# cube = platonic.Cube()\n",
    "# cube.vertices = np.array([\n",
    "#     [x_max, y_max, z_max],\n",
    "#     [x_max, y_max, z_min],\n",
    "#     [x_max, y_min, z_max],\n",
    "#     [x_max, y_min, z_min],\n",
    "#     [x_min, y_max, z_max],\n",
    "#     [x_min, y_max, z_min],\n",
    "#     [x_min, y_min, z_max],\n",
    "#     [x_min, y_min, z_min],\n",
    "#     ])\n",
    "# fig += k3d.mesh(cube.vertices, cube.indices, side=\"double\", opacity=.5, color=0xff0000)\n",
    "\n",
    "fig.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
